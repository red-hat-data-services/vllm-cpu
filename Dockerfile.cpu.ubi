# syntax=docker/dockerfile:1
# check=skip=InvalidDefaultArgInFrom # defaults are specified in bakefiles
# vim: ft=dockerfile

ARG BASE_UBI_IMAGE_TAG
ARG PYTHON_VERSION

## Base Layer ##################################################################
FROM registry.access.redhat.com/ubi9/ubi-minimal:${BASE_UBI_IMAGE_TAG} AS base
ARG PYTHON_VERSION
ENV PYTHON_VERSION=${PYTHON_VERSION}
RUN microdnf -y update && microdnf install -y \
    python${PYTHON_VERSION}-pip python${PYTHON_VERSION}-wheel \
    && microdnf clean all

WORKDIR /workspace

ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8

RUN microdnf install -y \
        which procps findutils tar \
        # required for vllm cpu
        numactl-libs \
        gcc \
        g++ \
    && microdnf clean all


## Python Installer ############################################################
FROM base AS python-install
ARG PYTHON_VERSION
ENV UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cpu
ENV UV_INDEX_STRATEGY=unsafe-best-match

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH="$VIRTUAL_ENV/bin:$PATH"
ENV PYTHON_VERSION=${PYTHON_VERSION}
RUN microdnf install -y \
    python${PYTHON_VERSION}-devel  && \
    python${PYTHON_VERSION} -m venv $VIRTUAL_ENV && \
    pip install --no-cache -U pip wheel uv && \
    microdnf clean all


## Release #####################################################################
FROM python-install AS vllm-openai
ARG PYTHON_VERSION

WORKDIR /workspace

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH="$VIRTUAL_ENV/bin:$PATH"


COPY LICENSE /licenses/vllm.md
COPY examples/*.jinja /app/data/template/
RUN mkdir -p /opt/app-root && \
    ln -s /app/data/template /opt/app-root/template
# install vllm (and any other dependencies) by running the payload script
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,src=payload,target=/workspace/payload \
    ./payload/run.sh

# VLLM_TARGET_DEVICE=cpu is required to force CPU platform
ENV HF_HUB_OFFLINE=1 \
    HOME=/home/vllm \
    VLLM_USAGE_STATS_SERVER=https://console.redhat.com/api/rhaiis-stats \
    VLLM_USAGE_SOURCE=quay-docker-image \
    OUTLINES_CACHE_DIR=/tmp/outlines \
    NUMBA_CACHE_DIR=/tmp/numba \
    TRITON_CACHE_DIR=/tmp/triton \
    VLLM_CACHE_ROOT=/tmp/vllm \
    VLLM_TARGET_DEVICE=cpu

# setup non-root user for OpenShift
RUN umask 002 && \
    useradd --uid 2000 --gid 0 vllm && \
    mkdir -p /home/vllm && \
    chmod g+rwx /home/vllm

# pre-download tiktoken/harmony tokenizer for disconnected environments
# openai_harmony uses TIKTOKEN_RS_CACHE_DIR
# while the python tiktoken module uses TIKTOKEN_CACHE_DIR
ENV TIKTOKEN_CACHE_DIR=${HOME}/.cache/tiktoken
ENV TIKTOKEN_RS_CACHE_DIR=${TIKTOKEN_CACHE_DIR}
RUN --mount=type=bind,src=tools/predownload_tiktoken_tokenizers.py,target=predownload_tiktoken_tokenizers.py \
    python predownload_tiktoken_tokenizers.py && \
    chmod g+rwx "${TIKTOKEN_CACHE_DIR}"

USER 2000
RUN echo "ulimit -c 0" >> /home/vllm/.bashrc
WORKDIR /home/vllm

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
