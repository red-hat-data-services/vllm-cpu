## Global Args ##################################################################
ARG BASE_UBI_IMAGE_TAG
ARG PYTHON_VERSION

FROM registry.access.redhat.com/ubi9/ubi-minimal:${BASE_UBI_IMAGE_TAG} AS base

ARG PYTHON_VERSION

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

RUN --mount=type=cache,target=/root/.cache/pip \
 microdnf -y update && \
 microdnf install -y --setopt=install_weak_deps=0 --nodocs \
    python${PYTHON_VERSION}-devel \
    python${PYTHON_VERSION}-pip \
    python${PYTHON_VERSION}-wheel && \
    python${PYTHON_VERSION} -m venv $VIRTUAL_ENV && \
    pip install -U pip wheel setuptools uv && \
 microdnf clean all


FROM base AS rocm_base
ARG ROCM_VERSION
ARG PYTHON_VERSION
ARG BASE_UBI_IMAGE_TAG

RUN printf "[amdgpu]\n\
name=amdgpu\n\
baseurl=https://repo.radeon.com/amdgpu/${ROCM_VERSION}/rhel/${BASE_UBI_IMAGE_TAG/-*/}/main/x86_64/\n\
enabled=1\n\
priority=50\n\
gpgcheck=1\n\
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key\n\
[ROCm-${ROCM_VERSION}]\n\
name=ROCm${ROCM_VERSION}\n\
baseurl=https://repo.radeon.com/rocm/rhel9/${ROCM_VERSION}/main\n\
enabled=1\n\
priority=50\n\
gpgcheck=1\n\
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key" > /etc/yum.repos.d/amdgpu.repo


RUN --mount=type=cache,target=/root/.cache/uv \
    export version="$(awk -F. '{print $1"."$2}' <<< $ROCM_VERSION)" && \
    uv pip install --pre \
        --index-url "https://download.pytorch.org/whl/rocm${version}" \
        torch==2.7.0+rocm${version}\
        torchvision==0.22.0+rocm${version} && \
    microdnf install -y --nodocs \
        # aiter requires rocm dev dependencies (and temporarily, which, see https://github.com/ROCm/aiter/pull/932)
        which \
        amd-smi-lib \
        hipblas-devel \
        hipblaslt-devel \
        hipcc \
        hipcub-devel \
        hipfft-devel \
        hiprand-devel \
        hipsolver-devel \
        hipsparse-devel \
        hsa-rocr-devel \
        miopen-hip-devel \
        rccl-devel \
        rocblas-devel \
        rocm-device-libs \
        rocprim-devel \
        rocrand-devel \
        rocthrust-devel \
        # Install libdrm-amdgpu to avoid errors when retrieving device information (amdgpu.ids: No such file or directory)
        libdrm-amdgpu \
        && \
    microdnf clean all

ENV PATH=/opt/rocm/bin:$PATH \
    LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/numpy.libs"
ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/pillow.libs:$LD_LIBRARY_PATH"
ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/triton/backends/amd/lib:$LD_LIBRARY_PATH"
ENV LD_LIBRARY_PATH="$VIRTUAL_ENV/lib/python${PYTHON_VERSION}/site-packages/torch/lib:$LD_LIBRARY_PATH"

RUN echo $LD_LIBRARY_PATH | tr : \\n >> /etc/ld.so.conf.d/torch-venv.conf && \
    ldconfig

FROM rocm_base as build_amdsmi

RUN microdnf -y install \
    amd-smi-lib && \
    microdnf clean all

WORKDIR /opt/rocm/share/amd_smi

RUN python setup.py bdist_wheel --dist-dir=/dist/

##################################################################################################

FROM rocm_base AS vllm-openai
ARG PYTHON_VERSION

WORKDIR /workspace

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH=$VIRTUAL_ENV/bin:$PATH

# Required for triton
RUN microdnf install -y --setopt=install_weak_deps=0 --nodocs gcc rsync && \
    microdnf clean all

RUN --mount=type=bind,from=build_amdsmi,src=/dist,target=/install/amdsmi/ \
    --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,src=payload,target=/workspace/payload \
    ./payload/run.sh

# temporary workaround until https://github.com/openai/tiktoken/pull/446 is merged
COPY make-blobfile-optional-in-tiktoken.patch /tmp/
RUN microdnf install -y patch && \
    patch -p 1 "$VIRTUAL_ENV/lib/python3.12/site-packages/tiktoken/load.py" /tmp/make-blobfile-optional-in-tiktoken.patch && \
    microdnf remove -y patch && \
    microdnf clean all

ENV HF_HUB_OFFLINE=1 \
    HOME=/home/vllm \
    VLLM_USAGE_STATS_SERVER=https://console.redhat.com/api/rhaiis-stats \
    VLLM_USAGE_SOURCE=quay-docker-image \
    # Silences the HF Tokenizers warning
    TOKENIZERS_PARALLELISM=false  \
    RAY_EXPERIMENTAL_NOSET_ROCR_VISIBLE_DEVICES=1 \
    VLLM_USE_TRITON_FLASH_ATTN=0 \
    VLLM_USE_V1=1 \
    HIP_FORCE_DEV_KERNARG=1 \
    OUTLINES_CACHE_DIR=/tmp/outlines \
    NUMBA_CACHE_DIR=/tmp/numba \
    TRITON_CACHE_DIR=/tmp/triton

# setup non-root user for OpenShift
RUN umask 002 && \
    useradd --uid 2000 --gid 0 vllm && \
    mkdir -p /licenses /home/vllm && \
    chmod g+rwx /home/vllm

COPY LICENSE /licenses/vllm.md
COPY examples/*.jinja /app/data/template/
RUN mkdir -p /opt/app-root && \
    ln -s /app/data/template /opt/app-root/template

USER 2000
WORKDIR /home/vllm

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
